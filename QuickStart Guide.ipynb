{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c890b277",
   "metadata": {},
   "source": [
    "# Quick Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "360e4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78452719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a5f63dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd3c239",
   "metadata": {},
   "source": [
    "# Building A Language Model Application\n",
    "\n",
    "## LLMS: Get predications from a language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d55616a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25571d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ce828a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. India \n",
      "2. Thailand \n",
      "3. California \n",
      "4. New York City \n",
      "5. Italy\n"
     ]
    }
   ],
   "source": [
    "text = \"What are 5 vacation destinations for someone who likes to eat vegetarian food?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1838c68",
   "metadata": {},
   "source": [
    "## Prompt Templates: Manage prompts for LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1486573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eff056fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"country\"],\n",
    "    template= \"GDP of {country}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99353021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GDP of India.\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(country =\"India\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0279a404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The GDP of India in 2020 was estimated at US$2.94 trillion. This places India in the fifth position among the world's largest economies, behind the United States, China, Japan, and Germany. India's economy is expected to grow in 2021, with the IMF forecasting a 7.4% rise in GDP.\n"
     ]
    }
   ],
   "source": [
    "print(llm(prompt.format(country =\"India\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04632d4",
   "metadata": {},
   "source": [
    "## Chains: Combine LLMs and prompts in multi-step workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a98dc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce59246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"country\"],\n",
    "    template= \"GDP of {country}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d309a9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm = llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fc93f7be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The Gross Domestic Product (GDP) of Nepal in 2020 is estimated to be around $29.71 billion USD.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(\"Nepal\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d73a24",
   "metadata": {},
   "source": [
    "## Agents: Dyanmically call chains based on user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "788524b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-search-results\n",
      "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests in c:\\users\\anandvarrier\\anaconda3\\lib\\site-packages (from google-search-results) (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\anandvarrier\\anaconda3\\lib\\site-packages (from requests->google-search-results) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\anandvarrier\\anaconda3\\lib\\site-packages (from requests->google-search-results) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anandvarrier\\anaconda3\\lib\\site-packages (from requests->google-search-results) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anandvarrier\\anaconda3\\lib\\site-packages (from requests->google-search-results) (3.4)\n",
      "Building wheels for collected packages: google-search-results\n",
      "  Building wheel for google-search-results (setup.py): started\n",
      "  Building wheel for google-search-results (setup.py): finished with status 'done'\n",
      "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32077 sha256=e479b49e8d6148038acfc60e26b4562a3e72ccde23d6869767f22900a7155318\n",
      "  Stored in directory: c:\\users\\anandvarrier\\appdata\\local\\pip\\cache\\wheels\\de\\1b\\db\\6474e3f9e34a03ca54b85d98fc7742001e6fae1ff3881e3ed4\n",
      "Successfully built google-search-results\n",
      "Installing collected packages: google-search-results\n",
      "Successfully installed google-search-results-2.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43669386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9773ed5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's load the language model we're going to use to control the agent.\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2402cffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"22cbf5dd3c328d2423fd8c0b7f027afbb1a927a44a08b8bbd388049ad3022703\"\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41cf2689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10f9739e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who owns the company and when it was established, as well as the next leap year.\n",
      "Action: Search\n",
      "Action Input: \"Sphinx Worldbiz Limited\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mSphinx Worldbiz Limited is a modern engineering company, dedicated to providing high quality design and product development services for the engineering ...\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out more specific information about the company\n",
      "Action: Search\n",
      "Action Input: \"Sphinx Worldbiz Limited owner\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mNarendra Surana - Founder\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out when the company was established\n",
      "Action: Search\n",
      "Action Input: \"Sphinx Worldbiz Limited founding date\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m29 Jan, 1996\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out the next leap year\n",
      "Action: Calculator\n",
      "Action Input: \"Next leap year\"\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 2024\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Narendra Surana is the owner of Sphinx Worldbiz Limited, which was established on 29 Jan, 1996. The next leap year is 2024.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Narendra Surana is the owner of Sphinx Worldbiz Limited, which was established on 29 Jan, 1996. The next leap year is 2024.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Who is the owner of Sphinx Worldbiz Limited? When was the company established and which is the next leap year?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5ca5c0",
   "metadata": {},
   "source": [
    "## Memory: Add State to Chains and agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d71a69f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f85195b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! It's nice to meet you. My name is AI. What's your name?\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd8009b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. My name is AI. What's your name?\n",
      "Human: I am Anand. Can you tell me how many liters of water should I have?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi Anand! That's a great question. The recommended amount of water to drink per day is 8-12 8-ounce glasses, which is about 2-3 liters.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input = \"I am Anand. Can you tell me how many liters of water should I have?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "096147d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. My name is AI. What's your name?\n",
      "Human: I am Anand. Can you tell me how many liters of water should I have?\n",
      "AI:  Hi Anand! That's a great question. The recommended amount of water to drink per day is 8-12 8-ounce glasses, which is about 2-3 liters.\n",
      "Human: That was informative. Do you beleive in God?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I'm sorry, Anand, I don't have an opinion on that.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input = \"That was informative. Do you beleive in God?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d976586a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. My name is AI. What's your name?\n",
      "Human: I am Anand. Can you tell me how many liters of water should I have?\n",
      "AI:  Hi Anand! That's a great question. The recommended amount of water to drink per day is 8-12 8-ounce glasses, which is about 2-3 liters.\n",
      "Human: That was informative. Do you beleive in God?\n",
      "AI:  I'm sorry, Anand, I don't have an opinion on that.\n",
      "Human: What is the second question I asked you and what is the alternative phrase for the first thing I said to you?\n",
      "AI:  The second question you asked me was \"Do you believe in God?\" and the alternative phrase for the first thing you said to me was \"Hello.\"\n",
      "Human: What is the second question I asked you and what is the alternative phrase for the second thing I said to you?\n",
      "AI:  The second question you asked me was \"Do you believe in God?\" and the alternative phrase for the second thing you said to me was \"I am Anand.\"\n",
      "Human: What is the second question I asked you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The second question you asked me was \"Do you believe in God?\"'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input = \"What is the second question I asked you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e914532d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. My name is AI. What's your name?\n",
      "Human: I am Anand. Can you tell me how many liters of water should I have?\n",
      "AI:  Hi Anand! That's a great question. The recommended amount of water to drink per day is 8-12 8-ounce glasses, which is about 2-3 liters.\n",
      "Human: That was informative. Do you beleive in God?\n",
      "AI:  I'm sorry, Anand, I don't have an opinion on that.\n",
      "Human: What is the second question I asked you and what is the alternative phrase for the first thing I said to you?\n",
      "AI:  The second question you asked me was \"Do you believe in God?\" and the alternative phrase for the first thing you said to me was \"Hello.\"\n",
      "Human: What is the second question I asked you and what is the alternative phrase for the second thing I said to you?\n",
      "AI:  The second question you asked me was \"Do you believe in God?\" and the alternative phrase for the second thing you said to me was \"I am Anand.\"\n",
      "Human: What is the second question I asked you?\n",
      "AI:  The second question you asked me was \"Do you believe in God?\"\n",
      "Human: What is the alternative of third question I asked you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' The alternative of the third question you asked me is \"What is the alternative phrase for the second thing I said to you?\"'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input = \"What is the alternative of third question I asked you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2ab72c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
